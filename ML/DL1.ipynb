{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#Softmax vs crossentropy\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#tâche de classification single modèle\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#à résultat équivalent, le modèle le plus rapide est tjrs celui à sélectionner\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "#[Exemple] Softmax + Crossentropy versus Tanh + MSE.py\n",
    "\n",
    "import keras\n",
    "\n",
    "\n",
    "#Softmax vs crossentropy\n",
    "#tâche de classification single modèle\n",
    "\n",
    "#à résultat équivalent, le modèle le plus rapide est tjrs celui à sélectionner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1961354951.py, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 15\u001b[0;36m\u001b[0m\n\u001b[0;31m    keras.layers.Dense(units:32, activation = keras.activations.tanh),\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#import du dataset fashion list, 10 classes\n",
    "\n",
    "#L'API de plus haut niveau dans Keras est C control\n",
    "\n",
    "\n",
    "#Dans Keras els combinaison lineaires sont nommées dense\n",
    "\n",
    "def run():\n",
    "    for seed in [42, 51, 89, 66, 16]:\n",
    "        (x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "        \n",
    "        model = keras.models.Sequential(\n",
    "            #Ici trois modèles ayant un nombre unit de couches différent sont définies\n",
    "            keras.layers.Flatten(),\n",
    "            keras.layers.Dense(units:32, activation = keras.activations.tanh),\n",
    "            keras.layers.Dense(units:16, activation = keras.activations.tanh),\n",
    "            keras.layers.Dense(units:10, activation = keras.activations.tanh)\n",
    "        )\n",
    "\n",
    "\n",
    "#Standardisation des données d'input\n",
    "x_train = (x_train - np.mean(x_train)/ np.std(x_train))\n",
    "\n",
    "#Important de normaliser av valeurs de x_train car les données du dataset peuvent fortement diverger si le split fait à l'arrache\n",
    "x_test  = (x_test - np.mean(x_train)/ np.std(x_train))\n",
    "\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes:10) * 2.0 - 1.0\n",
    "y_test = keras.utils.to_categorical(y_train, num_classes:10) * 2.0 - 1.0\n",
    "\n",
    "\n",
    "model.compile(loss=keras.losses.mean_squared_error, optimizer = keras.optimizers.SGD())\n",
    "\n",
    "model.fit(x_train, y_train, batch_size: 32, epochs:100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dataset de validation doit être différent du dataset de test afin d'éviter de l'overfit (test final (ne doit être utilisé qu'une seule fois))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mesure la plu sutilisée pour calculer la distance entre deux lois de probabilité\n",
    "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\n",
    "\n",
    "Cepandant elle n'est pas symétrique \n",
    "\n",
    "Son inverse vaut - categorical cross entropy?\n",
    "\n",
    "\n",
    "Lorsqu'on compare les modèles comparer les loss ne sert à riean \n",
    "\n",
    "softmax avec mse au lieu de la categorical crossentropy?? \n",
    "Quelle est la meilleur en fonction d'activation de la dernière couche/fonction de loss choisie??\n",
    "\n",
    "Chaque combinaison fonction d'activation, fonction de loss est important\n",
    "\n",
    "Comment initialiser les poids d'un réseau de neurone?\n",
    "\n",
    "Cela dépend de la fonction d'activation choisie encore une fois, Bien choisir son initializer!\n",
    "\n",
    "Cross entropy meilleur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
